{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f640f4-5cf5-489d-8ae9-2b0b69a2f07a",
   "metadata": {},
   "source": [
    "# Business Problem:\n",
    "Hospitals and diagnostic centers generate vast amounts of unstructured medical text data, including clinical notes, pathology reports, and radiology findings. Manually reviewing these documents to detect brain tumors is a time-consuming and error-prone task that requires specialized medical expertise. As the volume of patient data increases, healthcare professionals face growing challenges in efficiently processing and analyzing this information.\n",
    "\n",
    "This project aims to address the manual bottleneck by automating the classification of medical text reports related to brain tumors using Convolutional Neural Networks (CNNs). By leveraging deep learning techniques, the model can automatically detect the presence and type of brain tumors, significantly enhancing diagnostic accuracy and speed. This solution empowers healthcare providers to:\n",
    "\n",
    "**Reduce human error** in identifying brain tumor cases from complex medical reports.\n",
    "\n",
    "**Streamline diagnosis workflows**, allowing clinicians and radiologists to focus on treatment rather than document categorization.\n",
    "\n",
    "**Accelerate treatment planning** by providing faster and more reliable tumor detection, improving overall patient outcomes and operational efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30fdad14-3b31-433e-99f0-11d18ee0b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9f612e-aef5-4a0b-b84b-2a0e665f54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0780904-06f0-4fa3-b838-4126db48f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1/255,\n",
    "                                  shear_range =0.2,\n",
    "                                  zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641c3f11-7557-48e6-aba8-469a66c5a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5542 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(r'D:\\Brain Tumer Image\\dataset\\train',\n",
    "                                                target_size = (64, 64),\n",
    "                                                 color_mode='grayscale',  # ← this is the key change\n",
    "                                                 class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f177908-d7bd-4e2c-8fb2-35e1a2c9ebca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_tumor': 0, 'tumor': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56cf609d-00e2-4f91-90be-0b29cfc53af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1404 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale =1/255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(r'D:\\Brain Tumer Image\\dataset\\validation',\n",
    "                                           target_size = (64,64),\n",
    "                                            color_mode='grayscale',  # ← match here too\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3f4d7-68d2-48b3-b1e1-45bc648a46f7",
   "metadata": {},
   "source": [
    "# Modelling - Convolution Neural Network\n",
    "\n",
    "**Initialising the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abada299-fb68-431a-ad0c-9ab41e7e78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd0fdf-7601-4061-9873-081288151efa",
   "metadata": {},
   "source": [
    "**Step 1 - Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4961c74d-cd10-4734-832e-145ba80196eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAGARJUNA\\anaconda3\\nagaraju\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "classifier.add(Conv2D(input_shape=[64, 64, 1],\n",
    "                      filters = 32, kernel_size =3, activation ='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29a5cb-3893-4872-987f-159648864392",
   "metadata": {},
   "source": [
    "**Step 2 - MaxPooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790955f1-dc54-4294-9981-90327db37667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8851bd3-08ac-4b72-9b06-c74bfcbab875",
   "metadata": {},
   "source": [
    "**Step 3 - Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383344a5-743c-47f7-9e6a-9388714c5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47c0f5-95d5-470d-8553-695122ec1e7f",
   "metadata": {},
   "source": [
    "**Step 4 - Full Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e355d6-2652-46c3-b38e-f0f000269581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# hidden layers with 128 neurons\n",
    "classifier.add(Dense(units= 128, activation='relu'))\n",
    "\n",
    "# Output Layer with 1 Neuron\n",
    "classifier.add(Dense(units =1, activation ='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b3d7f-d40a-43cd-a087-f7a8fba78610",
   "metadata": {},
   "source": [
    "**Training the CNN Model with train data & testing the Model with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73047138-5fe1-4c03-b01d-69cbb88f721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dcd3b1f-f84b-4eed-bf06-2f80a422807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAGARJUNA\\anaconda3\\nagaraju\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 3s/step - accuracy: 0.7567 - loss: 0.4993 - val_accuracy: 0.9110 - val_loss: 0.2292\n",
      "Epoch 2/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 234ms/step - accuracy: 0.9377 - loss: 0.1726 - val_accuracy: 0.8803 - val_loss: 0.2965\n",
      "Epoch 3/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 229ms/step - accuracy: 0.9505 - loss: 0.1469 - val_accuracy: 0.9601 - val_loss: 0.1213\n",
      "Epoch 4/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 231ms/step - accuracy: 0.9562 - loss: 0.1209 - val_accuracy: 0.9601 - val_loss: 0.1216\n",
      "Epoch 5/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 231ms/step - accuracy: 0.9671 - loss: 0.1034 - val_accuracy: 0.9430 - val_loss: 0.1497\n",
      "Epoch 6/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 232ms/step - accuracy: 0.9708 - loss: 0.0834 - val_accuracy: 0.9679 - val_loss: 0.1196\n",
      "Epoch 7/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 231ms/step - accuracy: 0.9692 - loss: 0.0868 - val_accuracy: 0.9494 - val_loss: 0.1439\n",
      "Epoch 8/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 233ms/step - accuracy: 0.9766 - loss: 0.0649 - val_accuracy: 0.9672 - val_loss: 0.1026\n",
      "Epoch 9/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 233ms/step - accuracy: 0.9795 - loss: 0.0631 - val_accuracy: 0.9701 - val_loss: 0.1008\n",
      "Epoch 10/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 235ms/step - accuracy: 0.9823 - loss: 0.0573 - val_accuracy: 0.9801 - val_loss: 0.0718\n",
      "Epoch 11/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 236ms/step - accuracy: 0.9810 - loss: 0.0561 - val_accuracy: 0.9815 - val_loss: 0.0768\n",
      "Epoch 12/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 237ms/step - accuracy: 0.9819 - loss: 0.0556 - val_accuracy: 0.9751 - val_loss: 0.0854\n",
      "Epoch 13/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 233ms/step - accuracy: 0.9877 - loss: 0.0406 - val_accuracy: 0.9651 - val_loss: 0.1240\n",
      "Epoch 14/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 235ms/step - accuracy: 0.9820 - loss: 0.0456 - val_accuracy: 0.9608 - val_loss: 0.1167\n",
      "Epoch 15/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 236ms/step - accuracy: 0.9862 - loss: 0.0367 - val_accuracy: 0.9758 - val_loss: 0.0799\n",
      "Epoch 16/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 234ms/step - accuracy: 0.9879 - loss: 0.0315 - val_accuracy: 0.9793 - val_loss: 0.0818\n",
      "Epoch 17/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 233ms/step - accuracy: 0.9935 - loss: 0.0234 - val_accuracy: 0.9665 - val_loss: 0.1153\n",
      "Epoch 18/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 233ms/step - accuracy: 0.9905 - loss: 0.0215 - val_accuracy: 0.9836 - val_loss: 0.0759\n",
      "Epoch 19/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 234ms/step - accuracy: 0.9917 - loss: 0.0225 - val_accuracy: 0.9779 - val_loss: 0.0943\n",
      "Epoch 20/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 236ms/step - accuracy: 0.9923 - loss: 0.0202 - val_accuracy: 0.9651 - val_loss: 0.1287\n",
      "Epoch 21/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 237ms/step - accuracy: 0.9948 - loss: 0.0155 - val_accuracy: 0.9181 - val_loss: 0.2393\n",
      "Epoch 22/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 234ms/step - accuracy: 0.9921 - loss: 0.0235 - val_accuracy: 0.9744 - val_loss: 0.0950\n",
      "Epoch 23/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 238ms/step - accuracy: 0.9936 - loss: 0.0189 - val_accuracy: 0.9858 - val_loss: 0.0642\n",
      "Epoch 24/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 235ms/step - accuracy: 0.9949 - loss: 0.0159 - val_accuracy: 0.9672 - val_loss: 0.1265\n",
      "Epoch 25/25\n",
      "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 234ms/step - accuracy: 0.9930 - loss: 0.0213 - val_accuracy: 0.9751 - val_loss: 0.1032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22b4d999b20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb73b7-6d07-438d-9df7-a87ad9ee9b71",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "- **Making a Single prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e99bd4b-49ba-472b-be83-d36022139e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "No Tumor Detected\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the image\n",
    "test_image = Image.open(r\"D:\\Brain Tumer Image\\Image (138).jpg\")\n",
    "test_image = test_image.convert('L')  # convert to grayscale\n",
    "test_image = test_image.resize((64, 64))\n",
    "\n",
    "test_image = np.array(test_image)\n",
    "test_image = test_image.reshape(64, 64, 1)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "test_image = test_image.astype('float32') / 255.0  # Normalize the image\n",
    "\n",
    "# Prediction\n",
    "result = classifier.predict(test_image)\n",
    "\n",
    "# Evaluation\n",
    "if result[0][0] == 1:\n",
    "    print(\"Tumor Detected\")\n",
    "else:\n",
    "    print(\"No Tumor Detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cff22-759c-4081-aa47-c835182e0659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
